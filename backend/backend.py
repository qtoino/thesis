# -*- coding: utf-8 -*-
"""Pytorch_backend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FplETlCgg-NhcHjAJDdKB4B2UE93sXfS
"""

import torch
import torch.nn as nn
import torchaudio
import torchaudio.functional as F
import torchaudio.transforms as T

#import lmdb
import pickle

from mpl_toolkits.axes_grid1 import make_axes_locatable
import matplotlib.pyplot as plt
import numpy as np

from scipy.io import wavfile

print(torch.__version__)

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
print(f"Using device {device}")

LMDB_DIR = "./content/drive/MyDrive/tese/ESC50/lmdb/spec"
PATH_BEST = "./ESC50/lmdb/model/best_model.pt"
MODEL_PATH = PATH_BEST

PATH_BEST_S = "./ESC50/lmdb/model_s/best_model.pt"
MODEL_PATH_S = PATH_BEST_S



SAMPLE_RATE = 441000
NUM_SAMPLES = 2**16-1 ##2**16-1 - aproximadamente 4 segundos ou 5*16000
n_fft = 1024
n_mel = 128            ##128
cached = False

n_layers_vae = 3
n_dimensions = 32
x_latent = int(n_mel/(2**n_layers_vae))
y_latent = int((2*(NUM_SAMPLES+1)/n_fft)/(2**n_layers_vae))
unflatten_size = (n_dimensions, x_latent, y_latent) ##(32, 16, 16)

latent_dims = 128

batch_size = 128

normalize_dataset_db = False
ref_level_db = 20
min_level_db = -100

class VariationalEncoder(nn.Module):
    def __init__(self, latent_dims, min_val, max_val):  
        super(VariationalEncoder, self).__init__()
        self.conv1 = nn.Conv2d(1, 8, 3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)
        self.batch2 = nn.BatchNorm2d(16)
        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=1)  
        self.linear1 = nn.Linear(n_dimensions*x_latent*y_latent, 1024)
        self.linear2 = nn.Linear(1024, latent_dims)
        self.linear3 = nn.Linear(1024, latent_dims)

        self.N = torch.distributions.Normal(0, 1)
        #self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU
        #self.N.scale = self.N.scale.cuda()
        self.kl = 0

        self.min_val = min_val
        self.max_val = max_val

    def normalize_function(self, spec):
        mean = (self.min_val + self.max_val) / 2
        scale = self.max_val - self.min_val
        return (spec - mean) / scale + 0.5

    def forward(self, x):
        #print("VariationalEncoder", flush = True)
        x = x.to(device)

        x = self.normalize_function(x)
        #print("x " + str(x.size()), flush = True)
        x = nn.functional.relu(self.conv1(x))
        #print("relu(conv1) " + str(x.size()), flush = True)
        x = nn.functional.relu(self.batch2(self.conv2(x)))
        #print("relu(batch2(conv2)) " + str(x.size()), flush = True)
        x = nn.functional.relu(self.conv3(x))
        #print("relu(conv3) " + str(x.size()), flush = True)
        x = torch.flatten(x, start_dim=1)
        #print("flatten() " + str(x.size()), flush = True)
        x = nn.functional.relu(self.linear1(x))
        #print("relu(linear1) " + str(x.size()), flush = True)
        mu =  self.linear2(x)
        sigma = torch.exp(self.linear3(x))
        z = mu.to(device) + sigma*self.N.sample(mu.shape).to(device)
        #print("z " + str(z.size()), flush = True)
        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()
        return z

class Decoder(nn.Module):
    
    def __init__(self, latent_dims, min_val, max_val):
        super().__init__()

        self.decoder_lin = nn.Sequential(
            nn.Linear(latent_dims, 1024),
            nn.ReLU(True),
            nn.Linear(1024, n_dimensions*x_latent*y_latent),
            nn.ReLU(True)
        )

        self.unflatten = nn.Unflatten(dim=1, unflattened_size=unflatten_size)

        self.decoder_conv = nn.Sequential(
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(8),
            nn.ReLU(True),
            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)
        )

        self.min_val = min_val
        self.max_val = max_val
    
    def denormalize_function(self, spec):
        mean = (self.min_val + self.max_val) / 2
        scale = self.max_val - self.min_val
        return (spec - 0.5) * scale + mean
        
    def forward(self, x):
        #print("\nDecoder")
        #print("x " + str(x.size()), flush = True)   
        x = self.decoder_lin(x)
        #print("d_linear " + str(x.size()), flush = True)
        x = self.unflatten(x)
        #print("unflatten " + str(x.size()), flush = True)
        x = self.decoder_conv(x)
        #print("d_conv" + str(x.size()), flush = True)
        x = torch.sigmoid(x)
        #print("sigmoid" + str(x.size()), flush = True)
        x = self.denormalize_function(x)
        return x

class VariationalAutoencoder(nn.Module):
    def __init__(self, latent_dims, min_val, max_val):
        super(VariationalAutoencoder, self).__init__()
        self.encoder = VariationalEncoder(latent_dims, min_val, max_val)
        self.decoder = Decoder(latent_dims, min_val, max_val)

    def forward(self, x):
        x = x.to(device)
        z = self.encoder(x)
        return self.decoder(z)

def read_value_lmdb(index, lmdb_name):
    # Open the LMDB environment
    env = lmdb.open(lmdb_name, readonly=True)

    with env.begin() as txn:
        lmdblength = txn.get(f"{index:08}".encode("ascii"))
        # Remember it's a Spec_Image object that is loaded
        spec_image = pickle.loads(lmdblength)
        len = int(spec_image) if lmdblength is not None else 0
    return env, len

def calculate_picel(stft, sr, times=None, freqs=None):
    if freqs is None:
        freqs = np.linspace(0, sr / 2, stft.shape[0])

    if times is None:
        times = np.arange(stft.shape[1])

    X, Y = np.meshgrid(times, freqs)
    stft_aux = stft
    if torch.is_tensor(stft):
        if(stft.is_cuda):
            stft_aux = stft.cpu()
        stft_aux = stft_aux.detach().numpy()

    return X, Y, stft_aux

def plot_stft(stft, sr, ax=None, fig=None, times=None, freqs=None, label=None):

    def add_colorbar(fig, ax, im):
        divider = make_axes_locatable(ax)
        cax = divider.append_axes("right", size="5%", pad=0.05)
        fig.colorbar(im, cax=cax, orientation="vertical")

    if fig is None:
        fig, ax = plt.subplots(1, 1, figsize=(6, 4))

    X, Y, stft_aux = calculate_picel(stft, sr, times, freqs)

    im0 = ax.pcolor(X, Y, stft_aux, cmap="magma")
    add_colorbar(fig, ax, im0)
    ax.set_title(label)
    ax.set_xlabel("Time (s)")
    ax.set_ylabel("Frequency (Hz)")

    return fig

def show_spec_batch(sample_batched, sr, samples):
    """Show image for a batch of samples."""
    #print(sample_batched[0].size())
    #TUDO BURRO NESTE LOOP
    #print(sample_batched)
    dims = []
    try:
        dims = list(sample_batched.size())
        #print(f'try {dims[0]}')
    except:
        dims = sample_batched.shape
        sample_batched = torch.tensor(sample_batched)
        #print(f'except {sample_batched.shape}')

    for i in range(dims[0]): 
        #image = torch.tensor(sample_batched[i])
        #print(type(sample_batched))
        #print(image)
        image_sq = torch.squeeze(sample_batched[i])
        #print(f'image_sq {image_sq.shape}')
        f = plot_stft(image_sq, sr)
        if samples > 0:
            if samples-1 <= i:
                break

def denormalize(S):
  return (((torch.clamp(S, -1, 1)+1.)/2.) * -min_level_db) + min_level_db

class Mel2Audio(torch.nn.Module):
    def __init__(
        self,
        sample_freq,
        n_fft,
        n_mel,
        normalize
    ):  
        super().__init__()

        self.sample_freq=sample_freq
        self.n_fft=n_fft
        self.n_mel=n_mel
        self.normalize = normalize

        self.inv_mel_scale = torchaudio.transforms.InverseMelScale(n_stft = n_fft//2+1, n_mels=n_mel, sample_rate=sample_freq)

        #default: 
        #  win_length = n_fft
        #  hop_length = win_length // 2
        self.grif_lim = torchaudio.transforms.GriffinLim(n_fft=n_fft) 
          

    def forward(self, melspec) -> np.array:

        melspec = torch.tensor(melspec).to(device)

        if self.normalize:
            melspec = denormalize(melspec)+ref_level_db

        melspec = torchaudio.functional.DB_to_amplitude(melspec, 1, 1)

        # Convert to power spectrogram
        spec = self.inv_mel_scale(melspec)

        # Convert to mel-scale
        wav = self.grif_lim(spec)

        return wav

# Instantiate a pipeline
rpipeline = Mel2Audio(SAMPLE_RATE, n_fft, n_mel, normalize_dataset_db)

# Move the computation graph to CUDA
rpipeline.to(device=torch.device(device), dtype=torch.float32)

### Set the random seed for reproducible results
torch.manual_seed(0)

min_val = -80
max_val = 30
#env_test, min_val = read_value_lmdb(-2, LMDB_DIR) 
#env_test, max_val = read_value_lmdb(-3, LMDB_DIR) 
#env_test.close()

vae = VariationalAutoencoder(latent_dims=latent_dims, min_val=min_val, max_val=max_val)

globaliter = 0
beta = 0.1
warmup=10000

try:
    #vae.load_state_dict(torch.load(PATH))
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    vae.load_state_dict(checkpoint['model_state_dict'])
    #optim.load_state_dict(checkpoint['optimizer_state_dict'], map_location=device) 
    n_epoch = checkpoint['epoch']
    val_loss = checkpoint['loss']
    vae.eval()
    print("Loaded model from " + MODEL_PATH)
    print(f'Number of epochs trained on: {n_epoch}')
    print(f'Validation loss: {val_loss}')
except:
    n_epoch = 0
    print("Training from scratch!")

print(f'Selected device: {device}')


vae.to(device)

vae.eval()







class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(VAE, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        
        # Encoder layers
        self.encoder_fc1 = nn.Linear(input_dim, hidden_dim)
        self.encoder_fc2 = nn.Linear(hidden_dim, latent_dim*2)
        
        # Decoder layers
        self.decoder_fc1 = nn.Linear(latent_dim, hidden_dim)
        self.decoder_fc2 = nn.Linear(hidden_dim, input_dim)
        
    def encode(self, x):
        x = nn.functional.relu(self.encoder_fc1(x))
        x = self.encoder_fc2(x)
        mu, logvar = torch.chunk(x, 2, dim=-1)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return z
    
    def decode(self, z):
        z = nn.functional.relu(self.decoder_fc1(z))
        x_hat = torch.sigmoid(self.decoder_fc2(z))
        return x_hat
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_hat = self.decode(z)
        return x_hat, mu, logvar

input_dim = latent_dims
hidden_dim = 64
latent_dim = 3

model = VAE(input_dim, hidden_dim, latent_dim)

try:
    #vae.load_state_dict(torch.load(PATH))
    checkpoint = torch.load(MODEL_PATH_S, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    #optim.load_state_dict(checkpoint['optimizer_state_dict'], map_location=device) 
    n_epoch_s = checkpoint['epoch']
    val_loss = checkpoint['loss']
    model.eval()
    print("Loaded model from " + MODEL_PATH_S)
    print(f'Number of epochs trained on: {n_epoch_s}')
    print(f'Validation loss: {val_loss}')
except:
    n_epoch_s = 0
    print("Training from scratch!")

print(f'Selected device: {device}')

model.to(device)

model.eval()

class GenerativeF():
    def __init__(self, generated_i):  
        super(GenerativeF, self).__init__()
        self.generated_i = generated_i

    def latent2coor(self, latent):
        mu, logvar = model.encode(latent)
        z = model.reparameterize(mu, logvar)
        z = z.detach().cpu().numpy()[0]
        coor = [z[0]*100, z[1]*100, z[2]*100]
        return coor

    def spec2audio(self, img_recon):
        audio = rpipeline(img_recon)

        wav_sq = torch.squeeze(audio)
        wave_audio = wav_sq.cpu().numpy()
        # Set the filename and sampling rate
        self.generated_i = self.generated_i+1
        filename = './ESC50/generated/my_generated_sound_'+ str(self.generated_i) + '.wav'
        sampling_rate = 16000 # For example
        wavfile.write(filename, sampling_rate, wave_audio)
        return filename
    

    def audioAsInput(self, x):  
        x_encoded = vae.encoder(x)
        # print(x_encoded.shape)
        img_recon = vae.decoder(x_encoded)
        # print(img_recon.shape)

        # show_spec_batch(img_recon, SAMPLE_RATE, -1)

        filename = self.spec2audio(img_recon)
        
        return filename


    def latentAsInput(self, x):

        img_recon = vae.decoder(x)
        print(img_recon.shape)

        return self.spec2audio(img_recon)


    def coorAsInput(self, coor):
        
        input = [[coor["x"]/100, coor["y"]/100, coor["z"]/100]]

        x = model.decode(torch.tensor(input).to(device))

        return self.latentAsInput(x)
        
        
        
